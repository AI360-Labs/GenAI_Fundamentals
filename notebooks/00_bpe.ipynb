{"cells":[{"cell_type":"markdown","id":"70beae67","metadata":{"id":"70beae67"},"source":["# ðŸ”¤ Byte Pair Encoding (BPE) Implementation\n","\n","**Byte Pair Encoding (BPE)** is a tokenization algorithm used by many modern language models to convert text into tokens. It iteratively merges the most frequent pair of bytes or characters in the text.\n","\n","## ðŸŽ¯ What You'll Learn\n","\n","1. Understanding BPE tokenization process\n","2. Converting text to bytes and tokens\n","3. Finding frequent character pairs\n","4. Merging pairs to build vocabulary\n","5. Encoding and decoding with BPE\n","\n","---\n","\n","## ðŸ“Š Project Overview\n","\n","**Goal:** Implement a simple BPE tokenizer from scratch.\n","\n","**Process:**\n","1. ðŸ“ Start with raw text\n","2. ðŸ”¢ Convert to bytes\n","3. ðŸ“ˆ Find most frequent pairs\n","4. ðŸ”„ Merge pairs iteratively\n","5. ðŸ—‚ï¸ Build vocabulary\n","\n","---\n","\n","## ðŸš€ Section 1: Initial Setup\n"]},{"cell_type":"code","execution_count":null,"id":"d718ea2c","metadata":{"id":"d718ea2c"},"outputs":[],"source":["MY_STRING = \"Hello there, running fastest to the tallest hills.\""]},{"cell_type":"markdown","id":"619f0acd","metadata":{"id":"619f0acd"},"source":["---\n","\n","## ðŸ”¢ Section 2: Converting Text to Bytes\n","\n","BPE starts by converting text to bytes (UTF-8 encoding), which gives us a list of integers (0-255) representing each character.\n"]},{"cell_type":"code","execution_count":null,"id":"7ab1d372","metadata":{"id":"7ab1d372"},"outputs":[],"source":["raw_bytes = MY_STRING.encode('utf-8')\n","raw_bytes"]},{"cell_type":"markdown","id":"28773608","metadata":{"id":"28773608"},"source":["---\n","\n","## ðŸ“Š Section 3: Building Token List\n","\n","Now we convert the bytes to a list of integers - this is our initial token sequence where each byte is a token.\n"]},{"cell_type":"code","execution_count":null,"id":"b2973f03","metadata":{"id":"b2973f03"},"outputs":[],"source":["tokens = list(map(int, raw_bytes))\n","tokens"]},{"cell_type":"markdown","id":"082fd348","metadata":{"id":"082fd348"},"source":["---\n","\n","## ðŸ” Section 4: Finding Frequent Pairs\n","\n","The core of BPE is finding the most frequent consecutive pair of tokens. We'll count all adjacent pairs and identify which pair appears most often.\n"]},{"cell_type":"code","execution_count":null,"id":"c33139e4","metadata":{"id":"c33139e4"},"outputs":[],"source":["def get_stats(tokens: list[int]) -> dict[tuple[int, int], int]:\n","    \"\"\"Count frequency of adjacent token pairs.\"\"\"\n","    stats = {}\n","    for tok1, tok2 in zip(tokens, tokens[1:]):\n","        if (tok1, tok2) in stats:\n","            stats[(tok1, tok2)] += 1\n","        else:\n","            stats[(tok1, tok2)] = 1\n","    return stats\n","\n","stats = get_stats(tokens)\n","top_pair = max(stats, key=stats.get)\n","print(f\"Most frequent pair: {top_pair} (appears {stats[top_pair]} times)\")\n","print(f\"As characters: '{chr(top_pair[0])}{chr(top_pair[1])}'\")\n","print(f\"\\nTop 5 pairs:\")\n","for pair, count in sorted(stats.items(), key=lambda x: x[1], reverse=True)[:5]:\n","    print(f\"  {pair} -> '{chr(pair[0])}{chr(pair[1])}': {count} times\")"]},{"cell_type":"markdown","id":"39e47fee","metadata":{"id":"39e47fee"},"source":["---\n","\n","## ðŸ”„ Section 5: Merging Token Pairs\n","\n","Once we find the most frequent pair, we merge it into a single new token.\n","\n","This function replaces all occurrences of the pair with a new token ID.\n"]},{"cell_type":"markdown","id":"176600bb","metadata":{"id":"176600bb"},"source":["### ðŸ” How the Merge Function Works\n","\n","The `merge()` function scans through the token list and performs the following steps:\n","\n","**Input Parameters:**\n","- `tokens`: List of current token IDs (e.g., `[72, 101, 108, 108, 111]`)\n","- `pair`: The pair to find and merge (e.g., `(108, 108)` representing \"ll\")\n","- `new_token_id`: The new ID to replace the pair (e.g., `256`)\n","\n","**Algorithm:**\n","1. **Iterate** through tokens using index `i`\n","2. **Check** if current position has the matching pair:\n","   - If `tokens[i]` matches `pair[0]` AND `tokens[i+1]` matches `pair[1]`\n","   - Then: Add `new_token_id` to result and skip both tokens (`i += 2`)\n","3. **Otherwise**: Copy the current token unchanged and move to next (`i += 1`)\n","4. **Return** the new token list with merged pairs\n","\n","**Example:**\n","- Input: `[72, 101, 108, 108, 111]` (represents \"Hello\")\n","- Pair to merge: `(108, 108)` (represents \"ll\")\n","- New token ID: `256`\n","- Output: `[72, 101, 256, 111]` (now \"He[ll]o\" where `[ll]` is token 256)\n","\n","**Why Token 256?**\n","- Tokens 0-255 are reserved for individual bytes (UTF-8 encoding)\n","- New merged tokens start from 256 onwards\n","- Each merge creates a new vocabulary entry\n"]},{"cell_type":"code","execution_count":null,"id":"86d41eec","metadata":{"id":"86d41eec"},"outputs":[],"source":["def merge(tokens: list[int], pair: tuple[int, int], new_token_id: int) -> list[int]:\n","    \"\"\"Merge all occurrences of a pair into a single new token.\"\"\"\n","    new_tokens = []\n","    i = 0\n","    while i < len(tokens):\n","        if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n","            new_tokens.append(new_token_id)\n","            i += 2\n","        else:\n","            new_tokens.append(tokens[i])\n","            i += 1\n","    return new_tokens\n","\n","# Test the merge function\n","new_token_id = 256\n","merged_tokens = merge(tokens, top_pair, new_token_id)\n","print(f\"Original token count: {len(tokens)}\")\n","print(f\"After merging: {len(merged_tokens)}\")\n","print(f\"Tokens saved: {len(tokens) - len(merged_tokens)}\")\n"]},{"cell_type":"markdown","id":"bb3384ec","metadata":{"id":"bb3384ec"},"source":["---\n","\n","## ðŸ” Section 6: Iterative BPE Training\n","\n","Now we'll implement the full BPE training loop that repeatedly finds and merges the most frequent pairs until we reach our desired vocabulary size.\n"]},{"cell_type":"code","execution_count":null,"id":"32ef4415","metadata":{"id":"32ef4415"},"outputs":[],"source":["def train_bpe(text: str, num_merges: int) -> tuple[dict[int, tuple[int, int]], list[int]]:\n","    \"\"\"\n","    Train BPE tokenizer by performing specified number of merges.\n","\n","    Returns:\n","        merges: Dictionary mapping new token IDs to the pair they represent\n","        tokens: Final token sequence after all merges\n","    \"\"\"\n","    tokens = list(map(int, text.encode('utf-8')))\n","    merges = {}\n","\n","    for i in range(num_merges):\n","        stats = get_stats(tokens)\n","        if not stats:\n","            break\n","\n","        top_pair = max(stats, key=stats.get)\n","        new_token_id = 256 + i\n","        tokens = merge(tokens, top_pair, new_token_id)\n","        merges[new_token_id] = top_pair\n","\n","        if (i + 1) % 5 == 0 or i < 5:\n","            print(f\"Merge {i+1}: {top_pair} -> {new_token_id} (freq: {stats[top_pair]})\")\n","\n","    return merges, tokens\n","\n","# Train BPE with 20 merges\n","num_merges = 5\n","merges, final_tokens = train_bpe(MY_STRING, num_merges)\n","print(f\"\\nFinal token count: {len(final_tokens)} (original: {len(list(MY_STRING.encode('utf-8')))})\")\n","print(f\"Compression ratio: {len(final_tokens) / len(list(MY_STRING.encode('utf-8'))):.2%}\")\n"]},{"cell_type":"markdown","id":"aa5e906c","metadata":{"id":"aa5e906c"},"source":["---\n","\n","## ðŸ—‚ï¸ Section 7: Building Vocabulary\n","\n","Let's examine the vocabulary we've built. Each merge creates a new token that represents a pair of existing tokens.\n"]},{"cell_type":"code","execution_count":null,"id":"841740ef","metadata":{"id":"841740ef"},"outputs":[],"source":["def decode_token(token_id: int, merges: dict[int, tuple[int, int]]) -> bytes:\n","    \"\"\"Recursively decode a token ID back to bytes.\"\"\"\n","    if token_id < 256:\n","        return bytes([token_id])\n","\n","    pair = merges[token_id]\n","    return decode_token(pair[0], merges) + decode_token(pair[1], merges)\n","\n","print(\"Vocabulary (new tokens):\")\n","print(\"=\" * 60)\n","for token_id, pair in merges.items():\n","    decoded = decode_token(token_id, merges)\n","    print(f\"Token {token_id}: {pair} -> '{decoded.decode('utf-8', errors='replace')}'\")\n"]},{"cell_type":"markdown","id":"9cf789c7","metadata":{"id":"9cf789c7"},"source":["---\n","\n","## ðŸ”“ Section 8: Decoding Tokens Back to Text\n","\n","Now let's decode our tokenized sequence back to the original text to verify our implementation works correctly.\n"]},{"cell_type":"code","execution_count":null,"id":"02e8704f","metadata":{"id":"02e8704f"},"outputs":[],"source":["def decode(tokens: list[int], merges: dict[int, tuple[int, int]]) -> str:\n","    \"\"\"Decode a sequence of tokens back to text.\"\"\"\n","    byte_sequence = b\"\"\n","    for token in tokens:\n","        byte_sequence += decode_token(token, merges)\n","    return byte_sequence.decode('utf-8', errors='replace')\n","\n","decoded_text = decode(final_tokens, merges)\n","print(\"Original text:\")\n","print(f\"  '{MY_STRING}'\")\n","print(f\"\\n\\nTokenized text: {final_tokens} \\n\")\n","print(\"\\nDecoded text:\")\n","print(f\"  '{decoded_text}'\")\n","print(f\"\\nMatch: {MY_STRING == decoded_text}\")\n"]},{"cell_type":"markdown","id":"cc7fcecb","metadata":{"id":"cc7fcecb"},"source":["---\n","\n","## ðŸ” Section 9: Encoding New Text\n","\n","Let's create an encode function that can tokenize new text using our trained BPE vocabulary.\n"]},{"cell_type":"code","execution_count":null,"id":"cc4cc41f","metadata":{"id":"cc4cc41f"},"outputs":[],"source":["def encode(text: str, merges: dict[int, tuple[int, int]]) -> list[int]:\n","    \"\"\"Encode text using the trained BPE merges.\"\"\"\n","    tokens = list(map(int, text.encode('utf-8')))\n","\n","    while len(tokens) >= 2:\n","        stats = get_stats(tokens)\n","\n","        # Find the pair that should be merged based on our learned merges\n","        pair_to_merge = None\n","        for token_id in sorted(merges.keys()):\n","            pair = merges[token_id]\n","            if pair in stats:\n","                pair_to_merge = (pair, token_id)\n","                break\n","\n","        if pair_to_merge is None:\n","            break\n","\n","        pair, token_id = pair_to_merge\n","        tokens = merge(tokens, pair, token_id)\n","\n","    return tokens\n","\n","# Test encoding\n","test_text = \"Hello there\"\n","encoded = encode(test_text, merges)\n","decoded_back = decode(encoded, merges)\n","\n","print(f\"Test text: '{test_text}'\")\n","print(f\"Encoded tokens: {encoded}\")\n","print(f\"Token count: {len(encoded)}\")\n","print(f\"Decoded: '{decoded_back}'\")\n","print(f\"Match: {test_text == decoded_back}\")\n"]},{"cell_type":"markdown","id":"91f426f1","metadata":{"id":"91f426f1"},"source":["---\n","\n","## ðŸ“Š Section 10: Visualizing Tokenization\n","\n","Let's visualize how different texts are tokenized with our BPE vocabulary.\n"]},{"cell_type":"code","execution_count":null,"id":"47a47e9f","metadata":{"id":"47a47e9f"},"outputs":[],"source":["test_strings = [\n","    \"Hello there\",\n","    \"running fastest\",\n","    \"tallest hills\",\n","    \"Hello world\",\n","    \"testing BPE\"\n","]\n","\n","print(\"Tokenization comparison:\")\n","print(\"=\" * 70)\n","for text in test_strings:\n","    encoded = encode(text, merges)\n","    byte_count = len(text.encode('utf-8'))\n","    print(f\"\\nText: '{text}'\")\n","    print(f\"  Bytes: {byte_count}\")\n","    print(f\"  Tokens: {len(encoded)}\")\n","    print(f\"  Compression: {len(encoded)/byte_count:.2%}\")\n","    print(f\"  Token IDs: {encoded}\")\n"]},{"cell_type":"markdown","id":"4a153f2e","metadata":{"id":"4a153f2e"},"source":["---\n","\n","## ðŸ“š Summary\n","\n","### âœ¨ Key Concepts Covered\n","\n","1. **Byte Representation**: Converting text to bytes as the foundation of BPE\n","2. **Pair Frequency Analysis**: Counting adjacent token pairs to find patterns\n","3. **Iterative Merging**: Building vocabulary by repeatedly merging frequent pairs\n","4. **Token Vocabulary**: Creating new tokens that represent character sequences\n","5. **Encoding**: Converting text to tokens using learned merges\n","6. **Decoding**: Reconstructing original text from tokens\n","7. **Compression**: Reducing token count while preserving information\n","\n","### ðŸ’¡ Key Insights\n","\n","- âœ… **BPE is data-driven**: The vocabulary adapts to patterns in the training text\n","- âœ… **Hierarchical encoding**: New tokens build on previous merges\n","- âœ… **Compression vs expressiveness**: More merges = smaller sequences but larger vocabulary\n","- âœ… **Subword tokenization**: BPE naturally handles rare words by breaking them into subwords\n","- âœ… **Deterministic**: Same training data and merge count produces same vocabulary\n","\n","### ðŸŽ¯ Real-World Applications\n","\n","BPE is used in many modern language models:\n","- ðŸ”¹ **GPT models**: Use BPE tokenization\n","- ðŸ”¹ **BERT variants**: Many use WordPiece (similar to BPE)\n","- ðŸ”¹ **Translation models**: BPE helps handle rare words across languages\n","- ðŸ”¹ **Code models**: BPE adapts to programming syntax patterns\n","\n","### ðŸ“– Next Steps\n","\n","- ðŸ”¹ Train BPE on larger datasets\n","- ðŸ”¹ Experiment with different vocabulary sizes\n","- ðŸ”¹ Compare BPE with other tokenization methods\n","- ðŸ”¹ Implement BPE variants (e.g., byte-level BPE)\n","- ðŸ”¹ Explore tokenization in production models\n","\n","---\n","\n","### ðŸŽ“ Congratulations!\n","\n","You now understand how Byte Pair Encoding works and can implement a basic BPE tokenizer from scratch!\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.0"}},"nbformat":4,"nbformat_minor":5}