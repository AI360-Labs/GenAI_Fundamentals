{"cells":[{"cell_type":"markdown","id":"f57b8194","metadata":{"id":"f57b8194"},"source":["# üìã Creating Basic Prompts for Data Extraction\n","\n","**Prompting** is the art of crafting instructions for language models to get desired outputs. This notebook explores fundamental prompting techniques for extracting structured information from unstructured text.\n","\n","## üéØ What You'll Learn\n","\n","1. Loading and using language models for text extraction\n","2. Creating basic prompts for data extraction\n","3. Controlling output quality with generation parameters\n","4. Using repetition penalty to improve results\n","5. Enforcing structured output formats\n","6. Prompt engineering techniques\n","7. Understanding sampling vs greedy decoding\n","8. Fine-tuning model behavior with temperature, top-k, and top-p\n","\n","---\n","\n","## üìä Project Overview\n","\n","**Goal:** Extract structured information from resumes across different industries.\n","\n","**Target Fields:**\n","- üìù Name\n","- üè† Address\n","- üìû Phone number\n","- üìß Email\n","\n","**Sample Data:**\n","- Name: Tammy Jones\n","- Address: 4759 Sunnydale Lane, Plano, Texas, United States 75071\n","- Phone: 1234567890\n","- Email: youremailcom\n","\n","---\n","\n","## üöÄ Section 1: Setup and Model Loading\n"]},{"cell_type":"markdown","id":"36bbb9ed","metadata":{"id":"36bbb9ed"},"source":["### üì• Loading the Model\n","\n","We'll use the Qwen/Qwen2.5-0.5B model for text generation and information extraction."]},{"cell_type":"code","execution_count":null,"id":"142f6980","metadata":{"id":"142f6980"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n"]},{"cell_type":"code","execution_count":null,"id":"cb5c427e","metadata":{"id":"cb5c427e"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","torch.manual_seed(42)\n","model_name = \"Qwen/Qwen2.5-0.5B\"\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    dtype=\"auto\",\n","    device_map=\"auto\"\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n"]},{"cell_type":"code","execution_count":null,"id":"10bb8deb","metadata":{"id":"10bb8deb"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"2f07067c","metadata":{"id":"2f07067c"},"source":["---\n","\n","## üìÇ Section 2: Loading Resume Data\n","\n","### üíæ Loading the Dataset\n","\n","We'll load resume data from a CSV file containing text from various resumes."]},{"cell_type":"code","execution_count":null,"id":"2f9243f1","metadata":{"id":"2f9243f1"},"outputs":[],"source":["import pandas as pd\n","\n","cvs_df = pd.read_csv('https://raw.githubusercontent.com/AI360-Labs/GenAI_Fundamentals/refs/heads/main/data/resumes/resumes_scraped_sampled.csv')\n","cvs_df.head(3)"]},{"cell_type":"markdown","id":"7ed12c73","metadata":{"id":"7ed12c73"},"source":["### üî¢ Understanding Token Counts\n","\n","**Important:** Estimating token counts helps you understand model input/output limits.\n","\n","**Rules of Thumb:**\n","- 1 token ~= 4 chars in English\n","- 1 token ~= ¬æ words\n","- 100 tokens ~= 75 words"]},{"cell_type":"code","execution_count":null,"id":"bb708fe8","metadata":{"id":"bb708fe8"},"outputs":[],"source":["number_of_tokens = len(cvs_df.cv_text.values[0].split())\n","print(\"Approximate number of tokens in the prompt:\", number_of_tokens / 0.75)"]},{"cell_type":"markdown","id":"4f7dc34c","metadata":{"id":"4f7dc34c"},"source":["---\n","\n","## ‚úçÔ∏è Section 3: Creating Basic Prompts\n","\n","### üìù First Attempt: Simple Instruction Prompt\n","\n","Let's create a basic prompt that instructs the model to extract specific information."]},{"cell_type":"code","execution_count":null,"id":"22a68ec5","metadata":{"id":"22a68ec5"},"outputs":[],"source":["import torch\n","\n","prompt = \"\"\"I will provide you with a resume of a person.\n","Please extract the following information from the resume:\n","1. Name\n","2. Address\n","3. Phone number\n","4. Email\n","\n","Here is the resume:\n","{cv_text}\n","\"\"\"\n","\n","prompt = prompt.format(cv_text=cvs_df.cv_text.values[0])\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","outputs = model.generate(\n","    **inputs,\n","    max_new_tokens=100,\n","    pad_token_id=tokenizer.eos_token_id,\n","    )\n","\n","decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(decoded_output)"]},{"cell_type":"markdown","id":"93cc05bb","metadata":{"id":"93cc05bb"},"source":["### üîß Helper Function for Output Cleaning\n","\n","This function removes the initial prompt from the model's output to isolate the answer."]},{"cell_type":"code","execution_count":null,"id":"e51f75e6","metadata":{"id":"e51f75e6"},"outputs":[],"source":["def extract_answer_from_llm(prompt, decoded_output):\n","    \"\"\"Extracts the answer from the LLM output by removing initial prompt.\"\"\"\n","    if decoded_output.startswith(prompt):\n","        return decoded_output[len(prompt):].strip()\n","    else:\n","        return decoded_output"]},{"cell_type":"code","execution_count":null,"id":"7e27c082","metadata":{"id":"7e27c082"},"outputs":[],"source":["cleaned_output = extract_answer_from_llm(prompt=prompt, decoded_output=decoded_output)\n","print(cleaned_output)"]},{"cell_type":"markdown","id":"d73f0cbf","metadata":{"id":"d73f0cbf"},"source":["---\n","\n","## ‚öôÔ∏è Section 4: Controlling Output with Parameters\n","\n","### üö´ Repetition Penalty\n","\n","The repetition penalty parameter discourages the model from repeating the same words or phrases.\n","\n","- **Value = 1.0:** No penalty (default)\n","- **Value > 1.0:** Penalizes repetition (e.g., 1.1 reduces repetitive text)"]},{"cell_type":"code","execution_count":null,"id":"b10b0456","metadata":{"id":"b10b0456"},"outputs":[],"source":["prompt = \"\"\"I will provide you with a resume of a person.\n","Please extract the following information from the resume:\n","1. Name\n","2. Address\n","3. Phone number\n","4. Email\n","\n","Here is the resume:\n","{cv_text}\n","\"\"\"\n","\n","prompt = prompt.format(cv_text=cvs_df.cv_text.values[0])\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","outputs = model.generate(\n","    **inputs,\n","    max_new_tokens=100,\n","    pad_token_id=tokenizer.eos_token_id,\n","    repetition_penalty=1.1\n","    )\n","\n","decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","cleaned_output = extract_answer_from_llm(prompt=prompt, decoded_output=decoded_output)\n","print(cleaned_output)"]},{"cell_type":"markdown","id":"be3d63c4","metadata":{"id":"be3d63c4"},"source":["---\n","\n","## üéØ Section 5: Enforcing Concise Output\n","\n","### üìã Adding Output Constraints\n","\n","Let's refine our prompt to explicitly request less explanatory text and focus on the data."]},{"cell_type":"code","execution_count":null,"id":"7018c5d5","metadata":{"id":"7018c5d5"},"outputs":[],"source":["prompt = \"\"\"I will provide you with a resume of a person.\n","Please extract the following information from the resume:\n","1. Name\n","2. Address\n","3. Phone number\n","4. Email\n","\n","Here is the resume:\n","{cv_text}\n","\n","Do not provide any additional information except name, address, phone and email.\n","\"\"\"\n","\n","prompt = prompt.format(cv_text=cvs_df.cv_text.values[0])\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","outputs = model.generate(\n","    **inputs,\n","    max_new_tokens=100,\n","    pad_token_id=tokenizer.eos_token_id,\n","    repetition_penalty=1.1\n","    )\n","\n","decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","cleaned_output = extract_answer_from_llm(prompt=prompt, decoded_output=decoded_output)\n","print(cleaned_output)"]},{"cell_type":"markdown","id":"826e441b","metadata":{"id":"826e441b"},"source":["### ‚ùå Attempt: Further Restricting Explanations\n","\n","Adding more restrictions to prevent explanatory comments (Note: This may not always work)."]},{"cell_type":"code","execution_count":null,"id":"613e07f1","metadata":{"id":"613e07f1"},"outputs":[],"source":["prompt = \"\"\"I will provide you with a resume of a person.\n","Please extract the following information from the resume:\n","1. Name\n","2. Address\n","3. Phone number\n","4. Email\n","\n","Here is the resume:\n","{cv_text}\n","\n","Do not provide any additional information except name, address, phone and email.\n","Do not give additional commentary or explanation.\n","\"\"\"\n","\n","prompt = prompt.format(cv_text=cvs_df.cv_text.values[0])\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","outputs = model.generate(\n","    **inputs,\n","    max_new_tokens=100,\n","    pad_token_id=tokenizer.eos_token_id,\n","    repetition_penalty=1.1\n","    )\n","\n","decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","cleaned_output = extract_answer_from_llm(prompt=prompt, decoded_output=decoded_output)\n","print(cleaned_output)"]},{"cell_type":"markdown","id":"da7e9227","metadata":{"id":"da7e9227"},"source":["---\n","\n","## üí° Section 6: Advanced Prompting Technique - Priming\n","\n","### üé≤ Suggesting the Next Token\n","\n","Instead of allowing the model to start with phrases like \"Sure! Here is the answer\", we can **prime** the response by adding \"Answer:\" to the prompt. This forces the model to generate the actual answer as the next best token."]},{"cell_type":"code","execution_count":null,"id":"f0313708","metadata":{"id":"f0313708"},"outputs":[],"source":["prompt = \"\"\"I will provide you with a resume of a person.\n","Please extract the following information from the resume:\n","1. Name\n","2. Address\n","3. Phone number\n","4. Email\n","\n","Here is the resume:\n","{cv_text}\n","\n","Do not provide any additional information except name, address, phone and email.\n","Do not give additional commentary or explanation.\n","Answer:\n","\"\"\"\n","\n","prompt = prompt.format(cv_text=cvs_df.cv_text.values[0])\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","outputs = model.generate(\n","    **inputs,\n","    max_new_tokens=100,\n","    pad_token_id=tokenizer.eos_token_id,\n","    repetition_penalty=1.1\n","    )\n","\n","decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","cleaned_output = extract_answer_from_llm(prompt=prompt, decoded_output=decoded_output)\n","print(cleaned_output)"]},{"cell_type":"markdown","id":"cda4a41c","metadata":{"id":"cda4a41c"},"source":["---\n","\n","## üèóÔ∏è Section 7: Structured Prompt Engineering\n","\n","### üé® Refactoring for Better Results\n","\n","Applying best practices for prompt design:\n","\n","1. **Define Role & Goal:** Establishes context and pushes the model toward relevant token space\n","2. **Repeat Key Information:** Reinforces important instructions\n","3. **Use Special Tokens:** Leverage model-sensitive markers (XML tags, Markdown `###`, or model-specific tokens like `[/INST]`)"]},{"cell_type":"code","execution_count":null,"id":"30007d6d","metadata":{"id":"30007d6d"},"outputs":[],"source":["prompt = \"\"\"### Role:\n","You are an AI assistant specialized in extracting structured information from resumes.\n","Your goal is to locate relevant information in the provided resume text and return it in a JSON format exactly as it is in the document.\n","Return the information in a JSON format with the keys: \"name\", \"address\", \"phone\", \"email\".\n","\n","### Ask:\n","I will provide you with a resume of a person.\n","Please extract the information from the resume\n","\n","### Here is the resume:\n","{cv_text}\n","\n","### Return only JSON with the keys: \"name\", \"address\", \"phone\", \"email\".\n","### Answer:\n","\"\"\"\n","\n","prompt = prompt.format(cv_text=cvs_df.cv_text.values[0])\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","outputs = model.generate(\n","    **inputs,\n","    max_new_tokens=100,\n","    pad_token_id=tokenizer.eos_token_id,\n","    repetition_penalty=1.1,\n","    )\n","\n","decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","cleaned_output = extract_answer_from_llm(prompt=prompt, decoded_output=decoded_output)\n","print(cleaned_output)"]},{"cell_type":"markdown","id":"b0e650cd","metadata":{"id":"b0e650cd"},"source":["---\n","\n","## üé≤ Section 8: Sampling vs Greedy Decoding\n","\n","### üîç The `do_sample` Parameter\n","\n","The `do_sample` parameter controls how the model selects tokens:\n","\n","**`do_sample=False` (Default - Greedy Decoding):**\n","- Model always picks the most probable token\n","- Results are deterministic and consistent\n","- Less creative, more predictable\n","\n","**`do_sample=True` (Sampling):**\n","- Model samples from probability distribution\n","- Results vary between runs\n","- More creative and diverse\n","- Requires tuning with:\n","  - `temperature` (randomness)\n","  - `top_p` (nucleus sampling)\n","  - `top_k` (top-k sampling)"]},{"cell_type":"markdown","id":"9de59e0e","metadata":{"id":"9de59e0e"},"source":["### üéØ Experiment: Greedy Decoding (`do_sample=False`)\n","\n","**Key Observations:**\n","- Responses are **consistent** across all executions\n","- Values may be **formatted** rather than extracted exactly as written"]},{"cell_type":"code","execution_count":null,"id":"103012be","metadata":{"id":"103012be"},"outputs":[],"source":["prompt = \"\"\"### Role:\n","You are an AI assistant specialized in extracting structured information from resumes.\n","Your goal is to locate relevant information in the provided resume text and return it in a JSON format exactly as it is in the document.\n","Return the information in a JSON format with the keys: \"name\", \"address\", \"phone\", \"email\".\n","\n","### Ask:\n","I will provide you with a resume of a person.\n","Please extract the information from the resume\n","\n","### Here is the resume:\n","{cv_text}\n","\n","### Return only JSON with the keys: \"name\", \"address\", \"phone\", \"email\".\n","### Answer:\n","\"\"\"\n","\n","prompt = prompt.format(cv_text=cvs_df.cv_text.values[0])\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)"]},{"cell_type":"code","execution_count":null,"id":"ffdf27a5","metadata":{"id":"ffdf27a5"},"outputs":[],"source":["for iter in range(3):\n","    print(f\"Iteration {iter + 1}\")\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens=100,\n","        pad_token_id=tokenizer.eos_token_id,\n","        do_sample=False,\n","        )\n","\n","    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    cleaned_output = extract_answer_from_llm(prompt=prompt, decoded_output=decoded_output)\n","    print(cleaned_output, \"\\n\" + \"=\"*50 + \"\\n\")"]},{"cell_type":"markdown","id":"aa367580","metadata":{"id":"aa367580"},"source":["### üé® Experiment: Sampling (`do_sample=True`)\n","\n","**Key Observations:**\n","- Responses are **inconsistent** across executions\n","- Model may produce **more hallucinations**\n","- Results vary each time"]},{"cell_type":"code","execution_count":null,"id":"284acb01","metadata":{"id":"284acb01"},"outputs":[],"source":["for iter in range(3):\n","    print(f\"Iteration {iter + 1}\")\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens=100,\n","        pad_token_id=tokenizer.eos_token_id,\n","        do_sample=True,\n","        )\n","\n","    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    cleaned_output = extract_answer_from_llm(prompt=prompt, decoded_output=decoded_output)\n","    print(cleaned_output, \"\\n\" + \"=\"*50 + \"\\n\")"]},{"cell_type":"markdown","id":"4ab69bb6","metadata":{"id":"4ab69bb6"},"source":["---\n","\n","## üå°Ô∏è Section 9: Fine-Tuning with Temperature\n","\n","### üå°Ô∏è Temperature\n","\n","Temperature controls the model's **creativity** and **randomness**.\n","\n","**How it works:**\n","- Scales the probability distribution\n","- **Low temperature** (e.g., 0.01): Sharp distribution, more focused/deterministic\n","- **High temperature** (e.g., 1.5): Flat distribution, more diverse/creative\n","\n","**Note:** Even with very low temperature, if multiple tokens have similar probabilities after scaling, randomness still exists."]},{"cell_type":"code","execution_count":null,"id":"c96d1ccf","metadata":{"id":"c96d1ccf"},"outputs":[],"source":["for iter in range(3):\n","    print(f\"Iteration {iter + 1}\")\n","    print(\"-\"*15)\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens=100,\n","        pad_token_id=tokenizer.eos_token_id,\n","        do_sample=True,\n","        temperature=1e-6,  # Adjust temperature for randomness\n","        )\n","\n","    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    cleaned_output = extract_answer_from_llm(prompt=prompt, decoded_output=decoded_output)\n","    print(cleaned_output, \"\\n\")"]},{"cell_type":"markdown","id":"116f435f","metadata":{"id":"116f435f"},"source":["---\n","\n","## üéØ Section 10: Top-p (Nucleus) Sampling\n","\n","### üéØ Top-p Parameter\n","\n","Top-p fine-tunes token selection by limiting choices to the **smallest set whose cumulative probability sums to p**.\n","\n","- **High top-p** (e.g., 0.9): Wide range of word choices\n","- **Low top-p** (e.g., 0.01): Limits to most probable tokens only"]},{"cell_type":"code","execution_count":null,"id":"4eb4b7bb","metadata":{"id":"4eb4b7bb"},"outputs":[],"source":["for iter in range(3):\n","    print(f\"Iteration {iter + 1}\")\n","    print(\"-\"*15)\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens=100,\n","        pad_token_id=tokenizer.eos_token_id,\n","        do_sample=True,\n","        top_p=1e-2,  # Adjust top_p for more randomness\n","        )\n","\n","    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    cleaned_output = extract_answer_from_llm(prompt=prompt, decoded_output=decoded_output)\n","    print(cleaned_output, \"\\n\")"]},{"cell_type":"markdown","id":"c30b3e79","metadata":{"id":"c30b3e79"},"source":["---\n","\n","## üî¢ Section 11: Top-k Sampling\n","\n","### üî¢ Top-k Parameter\n","\n","Top-k limits the number of tokens considered for sampling.\n","\n","- **top_k = 1:** No sampling (greedy decoding - always picks most probable token)\n","- **top_k = 5:** Choose from top 5 most probable tokens\n","- **top_k = 50:** Choose from top 50 most probable tokens"]},{"cell_type":"code","execution_count":null,"id":"2bf3bbfa","metadata":{"id":"2bf3bbfa"},"outputs":[],"source":["for iter in range(3):\n","    print(f\"Iteration {iter + 1}\")\n","    print(\"-\"*15)\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens=100,\n","        pad_token_id=tokenizer.eos_token_id,\n","        do_sample=True,\n","        top_k=1,  # Adjust top_p for more randomness\n","        )\n","\n","    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    cleaned_output = extract_answer_from_llm(prompt=prompt, decoded_output=decoded_output)\n","    print(cleaned_output, \"\\n\")"]},{"cell_type":"markdown","id":"7f335831","metadata":{"id":"7f335831"},"source":["---\n","\n","## üìö Summary\n","\n","### ‚ú® Key Concepts Covered\n","\n","1. **Model Setup**: Loading and configuring language models for text extraction\n","2. **Basic Prompting**: Creating simple instruction-based prompts\n","3. **Token Counting**: Estimating input/output sizes\n","4. **Output Cleaning**: Removing prompts from model responses\n","5. **Repetition Penalty**: Preventing repetitive text generation\n","6. **Output Constraints**: Enforcing concise, focused responses\n","7. **Priming Technique**: Suggesting next tokens to guide output\n","8. **Structured Prompts**: Using roles, goals, and special tokens\n","9. **Sampling vs Greedy**: Understanding deterministic vs probabilistic decoding\n","10. **Temperature**: Controlling creativity and randomness\n","11. **Top-p Sampling**: Dynamic probability-based token selection\n","12. **Top-k Sampling**: Limiting token choices to top k options\n","\n","### üí° Best Practices\n","\n","- ‚úÖ **Define clear roles and goals** in prompts\n","- ‚úÖ **Repeat important instructions** for emphasis\n","- ‚úÖ **Use special tokens** (###, XML tags) for structure\n","- ‚úÖ **Prime responses** with keywords like \"Answer:\"\n","- ‚úÖ **Use low temperature** for factual extraction\n","- ‚úÖ **Apply repetition penalty** to avoid loops\n","- ‚úÖ **Combine parameters** for optimal results\n","\n","### üéØ Next Steps\n","\n","- üîπ Experiment with different prompt structures\n","- üîπ Try various parameter combinations\n","- üîπ Explore structured output formats (JSON)\n","- üîπ Compare different sampling strategies\n","- üîπ Build on these techniques for complex extraction tasks\n","\n","### üìñ Reference\n","\n","For more information on prompting parameters:\n","- [Prompt Engineering with Temperature and Top-p](https://promptengineering.org/prompt-engineering-with-temperature-and-top-p/#the-overlooked-power-of-llm-parameters-in-prompt-engineering)\n","\n","---\n","\n","### üéì Congratulations!\n","\n","You now understand the fundamentals of prompt engineering for data extraction and how to control model behavior using various generation parameters."]},{"cell_type":"markdown","id":"3e02bf46","metadata":{"id":"3e02bf46"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":5}