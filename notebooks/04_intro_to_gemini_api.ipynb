{"cells":[{"cell_type":"markdown","metadata":{"id":"7J3noNMfrwTG"},"source":["# üöÄ Introduction to Gemini API\n","\n","**Google's Gemini API** provides powerful access to large language models for text generation, analysis, and structured output. This notebook covers the essential concepts you'll need for the advanced sessions.\n","\n","## üéØ What You'll Learn\n","\n","1. Setting up the Gemini API client\n","2. Making basic API calls\n","3. Understanding configuration options\n","4. Using structured output with Pydantic\n","5. Error handling and best practices\n","6. Model selection and parameters\n","\n","---\n","\n","## üöÄ Section 1: Setup and Installation\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"9kMOOWR0rwTM","executionInfo":{"status":"ok","timestamp":1761640335381,"user_tz":-60,"elapsed":10189,"user":{"displayName":"Bartosz Kopytek","userId":"18071754892538977634"}}},"outputs":[],"source":["%pip install -U -q google-generativeai python-dotenv\n"]},{"cell_type":"markdown","metadata":{"id":"qUp3mGhArwTO"},"source":["---\n","\n","## üîë Section 2: API Key Setup\n","\n","### üì° Getting Your API Key\n","\n","You'll need a Google AI API key to use Gemini. Get one from [Google AI Studio](https://makersuite.google.com/app/apikey).\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"MHzwG2yVrwTO","executionInfo":{"status":"ok","timestamp":1761640346273,"user_tz":-60,"elapsed":845,"user":{"displayName":"Bartosz Kopytek","userId":"18071754892538977634"}}},"outputs":[],"source":["# For Google Colab - get API key from user data\n","from google.colab import userdata\n","api_key = userdata.get('GOOGLE_API_KEY_1')\n","\n","# Alternative: Set environment variable\n","# import os\n","# api_key = os.getenv('GOOGLE_API_KEY')\n"]},{"cell_type":"markdown","metadata":{"id":"KHiVJl5ErwTP"},"source":["---\n","\n","## üîå Section 3: Client Setup\n","\n","### üì° Initializing the Gemini Client\n","\n","The `genai.Client` is your main interface for interacting with Gemini models.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MuFwCIWprwTP","executionInfo":{"status":"ok","timestamp":1761640362634,"user_tz":-60,"elapsed":6047,"user":{"displayName":"Bartosz Kopytek","userId":"18071754892538977634"}},"outputId":"6382e554-9035-478b-c747-7c55e537e6c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Client initialized successfully!\n","Response: Hello! I'm a large language model, trained by Google.\n"]}],"source":["import os\n","from google import genai\n","from google.genai import types\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","# Initialize the client\n","MODEL_NAME = \"gemini-2.5-flash-lite\"\n","client = genai.Client(\n","    api_key=api_key\n",")\n","\n","# Test the connection\n","response = client.models.generate_content(\n","    model=MODEL_NAME,\n","    contents=\"Say hello and introduce yourself briefly\"\n",")\n","\n","print(\"Client initialized successfully!\")\n","print(f\"Response: {response.text}\")\n"]},{"cell_type":"markdown","metadata":{"id":"GIUvfDj3rwTQ"},"source":["---\n","\n","## üìû Section 4: Basic API Calls\n","\n","### üéØ Simple Text Generation\n","\n","The most basic way to interact with Gemini is through simple text prompts.\n"]},{"cell_type":"markdown","metadata":{"id":"XKTo0hLCrwTQ"},"source":["---\n","\n","## ‚öôÔ∏è Section 5: Configuration Options\n","\n","### üîß Using GenerateContentConfig\n","\n","The `types.GenerateContentConfig` class allows you to customize how Gemini responds to your prompts.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLSt_IxOrwTR","executionInfo":{"status":"ok","timestamp":1761506929890,"user_tz":-60,"elapsed":1056,"user":{"displayName":"Kateryna Jastrebowa","userId":"16258154105434962817"}},"outputId":"40567a38-0970-4699-e332-54656d32fede"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creative response (temperature=0.9): The future of AI will be an accelerating integration into all aspects of life, augmenting human capabilities and driving unprecedented innovation while simultaneously posing complex ethical and societal challenges.\n","\n","Conservative response (temperature=0.1): The future of AI will be characterized by increasingly sophisticated and integrated systems that augment human capabilities, automate complex tasks, and drive unprecedented innovation across all aspects of life.\n"]}],"source":["# Configuration with different parameters\n","prompt = \"Describe the future of AI in one sentence.\"\n","\n","# Creative response (high temperature)\n","creative_response = client.models.generate_content(\n","    model=MODEL_NAME,\n","    contents=prompt,\n","    config=types.GenerateContentConfig(\n","        temperature=0.9,\n","        max_output_tokens=100\n","    )\n",")\n","\n","# Conservative response (low temperature)\n","conservative_response = client.models.generate_content(\n","    model=MODEL_NAME,\n","    contents=prompt,\n","    config=types.GenerateContentConfig(\n","        temperature=0.1,\n","        max_output_tokens=100\n","    )\n",")\n","\n","print(f\"Creative response (temperature=0.9): {creative_response.text}\")\n","print(f\"\\nConservative response (temperature=0.1): {conservative_response.text}\")\n"]},{"cell_type":"markdown","metadata":{"id":"I2SE9o90rwTR"},"source":["### üìã Key Configuration Parameters\n","\n","| Parameter | Type | Description |\n","|-----------|------|-------------|\n","| `temperature` | float | Controls randomness of token selection. Lower values (closer to 0.0) produce more predictable output, higher values (closer to 2.0) result in more creative responses. Default: 1.0 |\n","| `topK` | int | Changes how model selects tokens by considering only the most probable tokens up to specified value. Value of 1 means model always selects most probable token |\n","| `topP` | float | Model selects tokens from most to least probable until cumulative probability equals specified value. Lower value = less random, higher value = more random (Nucleus sampling) |\n","| `maxOutputTokens` | int | Sets maximum number of tokens model can generate in single response, controlling output length |\n","| `stopSequences` | string[] | List of character sequences that cause model to stop generating content if generated. Useful for ending response at specific point |\n","| `candidateCount` | int | Sets number of possible responses to generate. Default returns response with highest probability |\n","| `thinkingBudget` | int | Available on Gemini 2.5 models. Controls number of \"thinking tokens\" for reasoning. Higher values allow more complex reasoning. Value of -1 enables dynamic thinking |\n","| `safetySettings` | object | Allows adjustment of probability threshold for blocking content in specific categories (DEROGATORY, TOXICITY, VIOLENCE, HARASSMENT) |\n","| `presencePenalty` | float | Discourages model from repeating tokens that have already appeared in generated text, promoting more diverse content |\n","| `frequencyPenalty` | float | Penalizes tokens that repeatedly appear in generated text, reducing probability of repeating content |\n","| `structuredOutput` | string or object | Configures model to generate structured output (e.g., JSON) that adheres to specific schema |\n","| `tuningParameters` | object | Specifies parameters for fine-tuning model on specific datasets (train_dataset, validation_dataset) |\n"]},{"cell_type":"markdown","metadata":{"id":"cSwfMBNgrwTS"},"source":["---\n","\n","## üé≠ Section 6: System Instructions\n","\n","### üìù Setting Context and Behavior\n","\n","System instructions help define the AI's role and behavior for your specific use case.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-J7zXmJrwTS","executionInfo":{"status":"ok","timestamp":1761506931462,"user_tz":-60,"elapsed":1567,"user":{"displayName":"Kateryna Jastrebowa","userId":"16258154105434962817"}},"outputId":"e91e86c7-9401-4869-8eed-0fdb6e034646"},"outputs":[{"output_type":"stream","name":"stdout","text":["Default response: Becoming a data scientist is a journey that involves acquiring a diverse set of skills and knowledge. Here's a comprehensive breakdown of what you should learn, categorized for clarity:\n","\n","## 1. Foundational Knowledge & Math\n","\n","This is the bedrock of data science. You don't need to be a mathematician, but a solid understanding of these concepts is crucial for understanding algorithms and interpreting results.\n","\n","*   **Linear Algebra:** Essential for understanding many machine learning algorithms (e.g., PCA,\n","========================================================================================================================================================================================================\n","\n","Expert response: It's fantastic that you're aiming to become a data scientist! It's a rewarding and in-demand field. To give you the most practical and actionable advice, I'll break it down into key areas. Think of this as a roadmap, not a rigid checklist. The journey is iterative, and you'll constantly be learning and refining your skills.\n","\n","Here's what you should focus on, categorized for clarity:\n","\n","## 1. Foundational Knowledge (The Bed\n"]}],"source":["# Without system instruction\n","default_response = client.models.generate_content(\n","    model=MODEL_NAME,\n","    contents=\"What should I learn to become a data scientist?\",\n","    config=types.GenerateContentConfig(\n","        max_output_tokens=100\n","    )\n",")\n","\n","# With system instruction\n","expert_response = client.models.generate_content(\n","    model=MODEL_NAME,\n","    contents=\"What should I learn to become a data scientist?\",\n","    config=types.GenerateContentConfig(\n","        system_instruction=\"You are an expert data scientist with 10+ years of experience. Provide practical, actionable advice.\",\n","        max_output_tokens=100\n","    )\n",")\n","\n","print(f\"Default response: {default_response.text}\")\n","print(f\"=\"*200)\n","print(f\"\\nExpert response: {expert_response.text}\")\n"]},{"cell_type":"markdown","metadata":{"id":"CTGlNkmnrwTS"},"source":["---\n","\n","## üìä Section 7: Structured Output\n","\n","### üéØ Using Pydantic Schemas\n","\n","Structured output ensures consistent, validated data formats from the AI.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHrtFL5jrwTT"},"outputs":[],"source":["from pydantic import BaseModel, Field\n","from datetime import date\n","from typing import Optional\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2vOiskkHrwTT","executionInfo":{"status":"ok","timestamp":1761506931509,"user_tz":-60,"elapsed":21,"user":{"displayName":"Kateryna Jastrebowa","userId":"16258154105434962817"}},"outputId":"3b918fb9-5879-48ac-8c9a-874fef8611b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Schema defined successfully!\n","Test person: {\n","  \"name\": \"Alice Johnson\",\n","  \"age\": 25,\n","  \"occupation\": \"Software Engineer\",\n","  \"birth_date\": null,\n","  \"is_student\": false\n","}\n"]}],"source":["# Define a simple schema for person information\n","class Person(BaseModel):\n","    name: str = Field(description=\"Full name of the person\")\n","    age: int = Field(description=\"Age in years\", ge=0, le=150)\n","    occupation: str = Field(description=\"Job or profession\")\n","    birth_date: Optional[date] = Field(description=\"Date of birth\", default=None)\n","    is_student: bool = Field(description=\"Whether the person is currently a student\")\n","\n","# Test the schema\n","test_person = Person(\n","    name=\"Alice Johnson\",\n","    age=25,\n","    occupation=\"Software Engineer\",\n","    is_student=False\n",")\n","\n","print(\"Schema defined successfully!\")\n","print(f\"Test person: {test_person.model_dump_json(indent=2)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3YHz6r29rwTT","executionInfo":{"status":"ok","timestamp":1761506932232,"user_tz":-60,"elapsed":722,"user":{"displayName":"Kateryna Jastrebowa","userId":"16258154105434962817"}},"outputId":"f6c85b2c-97d8-46b0-a958-2481d090f41c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Structured output:\n","{\n","  \"name\": \"Marie Curie\",\n","  \"age\": 66,\n","  \"occupation\": \"Physicist and Chemist\",\n","  \"birth_date\": \"1867-11-07\",\n","  \"is_student\": false\n","}\n"]}],"source":["# Extract structured information using the schema\n","prompt = \"\"\"\n","Extract information about Marie Curie and format it according to the Person schema.\n","She was born on November 7, 1867, and died at age 66.\n","She was a physicist and chemist, not a student.\n","\"\"\"\n","\n","response = client.models.generate_content(\n","    model=MODEL_NAME,\n","    contents=prompt,\n","    config=types.GenerateContentConfig(\n","        system_instruction=\"You are an expert at extracting structured information from text.\",\n","        response_mime_type=\"application/json\",\n","        temperature=0.01,\n","        response_schema=Person\n","    )\n",")\n","\n","# Parse the structured response\n","person_data = Person.model_validate_json(response.text)\n","\n","print(\"Structured output:\")\n","print(person_data.model_dump_json(indent=2))\n"]},{"cell_type":"code","source":[],"metadata":{"id":"W3SOJdow4bZl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üîß Section 8: Gemini Tool Calling\n","\n","### üéØ Overview\n","\n","Tool calling enables Gemini to interact with external functions during content generation. This allows the model to perform computations, access data, or trigger actions beyond its core text generation capabilities.\n","\n","**Key Concepts:**\n","- üîß **Function Declarations**: Define available functions with parameters and descriptions\n","- üéØ **Single Tool**: Use one function for specific operations\n","- üîÑ **Multiple Tools**: Combine multiple functions for complex workflows\n","- ‚öôÔ∏è **Manual Execution**: Handle function calls and process results programmatically\n","\n","---"],"metadata":{"id":"c3UOHZ3m4cb8"}},{"cell_type":"code","source":["def sum_numbers(a: float, b: float) -> float:\n","    \"\"\"\n","    Adds two numbers together.\n","\n","    Args:\n","        a: First number\n","        b: Second number\n","\n","    Returns:\n","        Sum of the two numbers\n","    \"\"\"\n","    return a + b\n","\n","\n","def divide_numbers(a: float, b: float) -> float:\n","    \"\"\"\n","    Divides the first number by the second.\n","\n","    Args:\n","        a: Numerator\n","        b: Denominator\n","\n","    Returns:\n","        Result of the division\n","    \"\"\"\n","    if b == 0:\n","        raise ValueError(\"Cannot divide by zero\")\n","    return a / b"],"metadata":{"id":"V83OFB-p4l80"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.genai import types\n","\n","# Declare functions:\n","sum_function = types.FunctionDeclaration(\n","    name=\"sum_numbers\",\n","    description=\"Adds two numbers together\",\n","    parameters={\n","        \"type\": \"object\",\n","        \"properties\": {\n","            \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n","            \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n","        },\n","        \"required\": [\"a\", \"b\"]\n","    }\n",")\n","\n","divide_function = types.FunctionDeclaration(\n","    name=\"divide_numbers\",\n","    description=\"Divides first number by second number\",\n","    parameters={\n","        \"type\": \"object\",\n","        \"properties\": {\n","            \"a\": {\"type\": \"number\", \"description\": \"The number to divide\"},\n","            \"b\": {\"type\": \"number\", \"description\": \"The number to divide by (must not be zero)\"}\n","        },\n","        \"required\": [\"a\", \"b\"]\n","    }\n",")"],"metadata":{"id":"2dkrz1Sc5rCS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["custom_tools = [\n","    types.Tool(function_declarations=[sum_function, divide_function])\n","]\n","\n","\n","prompt = \"What is the sum of 15 and 27?\"\n","#prompt = \"Sum 5+5 and divide by 2?\"\n","\n","response = client.models.generate_content(\n","    model=MODEL_NAME,\n","    contents=prompt,\n","    config=types.GenerateContentConfig(\n","        tools=custom_tools\n","    )\n",")\n","\n","print(response)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B46pAnR153j5","executionInfo":{"status":"ok","timestamp":1761506932790,"user_tz":-60,"elapsed":500,"user":{"displayName":"Kateryna Jastrebowa","userId":"16258154105434962817"}},"outputId":"38cbb063-4b79-4679-91c2-27457e9f22bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sdk_http_response=HttpResponse(\n","  headers=<dict len=10>\n",") candidates=[Candidate(\n","  content=Content(\n","    parts=[\n","      Part(\n","        function_call=FunctionCall(\n","          args={\n","            'a': 15,\n","            'b': 27\n","          },\n","          name='sum_numbers'\n","        )\n","      ),\n","    ],\n","    role='model'\n","  ),\n","  finish_reason=<FinishReason.STOP: 'STOP'>,\n","  index=0\n",")] create_time=None model_version='gemini-2.5-flash-lite' prompt_feedback=None response_id='dHb-aKmIKbmM_PUPtorAuAc' usage_metadata=GenerateContentResponseUsageMetadata(\n","  candidates_token_count=22,\n","  prompt_token_count=126,\n","  prompt_tokens_details=[\n","    ModalityTokenCount(\n","      modality=<MediaModality.TEXT: 'TEXT'>,\n","      token_count=126\n","    ),\n","  ],\n","  total_token_count=148\n",") automatic_function_calling_history=[] parsed=None\n"]}]},{"cell_type":"code","source":["if response.candidates and response.candidates[0].content.parts:\n","    for part in response.candidates[0].content.parts:\n","        if hasattr(part, 'function_call') and part.function_call:\n","            function_call = part.function_call\n","\n","            if function_call.name == \"divide_numbers\":\n","                function_result = divide_numbers(**function_call.args)\n","                print('Division was used')\n","                print(function_result)\n","            elif function_call.name == \"sum_numbers\":\n","                function_result = sum_numbers(**function_call.args)\n","                print('Sum was used')\n","                print(function_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_qDDkCVV7VQs","executionInfo":{"status":"ok","timestamp":1761506932818,"user_tz":-60,"elapsed":27,"user":{"displayName":"Kateryna Jastrebowa","userId":"16258154105434962817"}},"outputId":"4e0ad731-381f-4487-da77-60a4a9a07c8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sum was used\n","42\n"]}]},{"cell_type":"markdown","metadata":{"id":"_Wj8GMH0rwTU"},"source":["\n","\n","## ‚ö†Ô∏è Section 9: Error Handling\n","\n","### üõ°Ô∏è Robust API Usage\n","\n","Always handle potential errors when working with APIs.\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xoK-LJtArwTU","executionInfo":{"status":"ok","timestamp":1761506933307,"user_tz":-60,"elapsed":477,"user":{"displayName":"Kateryna Jastrebowa","userId":"16258154105434962817"}},"outputId":"41f66089-1287-4dad-b2f7-6cd1bceab4bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Success: The capital of France is **Paris**.\n","‚ùå Error: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/invalid-model-name is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\n"]}],"source":["def safe_api_call(prompt: str, model_name: str = MODEL_NAME) -> str:\n","    \"\"\"\n","    Safely call the Gemini API with error handling.\n","\n","    Args:\n","        prompt: The text prompt to send\n","        model_name: The model to use\n","\n","    Returns:\n","        Response text or error message\n","    \"\"\"\n","    try:\n","        response = client.models.generate_content(\n","            model=model_name,\n","            contents=prompt,\n","            config=types.GenerateContentConfig(\n","                temperature=0.1,\n","                max_output_tokens=100\n","            )\n","        )\n","        return f\"‚úÖ Success: {response.text}\"\n","\n","    except Exception as e:\n","        return f\"‚ùå Error: {str(e)}\"\n","\n","# Test successful call\n","result1 = safe_api_call(\"What is the capital of France?\")\n","print(result1)\n","\n","# Test error case\n","result2 = safe_api_call(\"Hello\", \"invalid-model-name\")\n","print(result2)\n"]},{"cell_type":"markdown","metadata":{"id":"a5BTGYxCrwTV"},"source":["---\n","\n","## üí° Section 10: Best Practices\n","\n","### ‚úÖ Essential Guidelines\n","\n","**1. Always Use Configuration Classes**\n","- Use `types.GenerateContentConfig()` instead of dictionaries\n","- Provides better type safety and IDE support\n","\n","**2. Set Appropriate Temperature**\n","- `0.0-0.3`: Factual, deterministic responses\n","- `0.4-0.7`: Balanced creativity and accuracy\n","- `0.8-1.0`: Creative, varied responses\n","\n","**3. Use System Instructions**\n","- Define the AI's role and context\n","- Improves consistency and relevance\n","\n","**4. Implement Error Handling**\n","- Always wrap API calls in try-catch blocks\n","- Provide meaningful error messages\n","\n","**5. Choose the Right Model**\n","- Use Flash Lite for simple, fast tasks\n","- Use Flash for complex, quality-focused tasks\n","\n","**6. Structure Your Output**\n","- Use Pydantic schemas for consistent data\n","- Set `response_mime_type=\"application/json\"`\n","\n","**7. Optimize Token Usage**\n","- Set `max_output_tokens` to control response length\n","- Monitor token consumption for cost management\n"]},{"cell_type":"markdown","metadata":{"id":"NTeYrW42rwTV"},"source":["---\n","\n","## üìö Summary\n","\n","### ‚ú® Key Concepts Covered\n","\n","1. **Client Setup**: Initializing the Gemini API client with proper authentication\n","2. **Basic Calls**: Making simple text generation requests\n","3. **Configuration**: Using `types.GenerateContentConfig` for customization\n","4. **System Instructions**: Setting context and behavior for the AI\n","5. **Structured Output**: Using Pydantic schemas for consistent data\n","6. **Error Handling**: Implementing robust API usage patterns\n","7. **Structured output**: Implementig, how to enforce structured output from the model\n","8. **Tool calling**: giving a model opportunity to identify whuch tools to use\n","9. **Error handling**: Examples of error handling\n","10. **Best Practices**: Following guidelines for optimal results\n","\n","### üéØ What's Next\n","\n","You're now ready for the advanced sessions:\n","\n","- **05_enforcing_structured_output.ipynb**: Deep dive into Pydantic schemas and validation\n","- **06_using_tools.ipynb**: Integrating external functions and APIs\n","- **07_building_agentic_workflow.ipynb**: Creating multi-step AI workflows\n","\n","### üí° Key Takeaways\n","\n","- ‚úÖ **Always use `types.GenerateContentConfig()`** for configuration\n","- ‚úÖ **Set appropriate temperature** based on your needs\n","- ‚úÖ **Use system instructions** to define AI behavior\n","- ‚úÖ **Implement error handling** for production code\n","- ‚úÖ **Choose the right model** for your use case\n","- ‚úÖ **Use structured output** for consistent data\n","- ‚úÖ **Follow best practices** for optimal results\n","\n","---\n","\n","### üéì Congratulations!\n","\n","You now have a solid foundation in using the Gemini API! You're ready to explore advanced features like structured output, tool integration, and agentic workflows.\n"]},{"cell_type":"markdown","metadata":{"id":"F6UTjml1rwTW"},"source":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}